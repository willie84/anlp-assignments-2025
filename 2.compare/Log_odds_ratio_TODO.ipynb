{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41mTyhrMogTX"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/dbamman/anlp25/blob/main/2.compare/Log_odds_ratio_TODO.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bea5Ilu7ogTY"
      },
      "source": [
        "# Log odds-ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIVpdSX0E9Tw"
      },
      "source": [
        "The log odds ratio with an informative (and uninformative) Dirichlet prior (described in [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf)) is a common method for finding distinctive terms in two datasets (see [Jurafsky et al. 2014](https://firstmonday.org/ojs/index.php/fm/article/view/4944/3863) for an example article that uses it to make an empirical argument). This method for finding distinguishing words combines a number of desirable properties:\n",
        "\n",
        "* it specifies an intuitive metric (the log-odds) for the ratio of two probabilities\n",
        "* it can incorporate prior information in the form of pseudocounts, which can either act as a smoothing factor (in the uninformative case) or incorporate real information about the expected frequency of words overall.\n",
        "* it accounts for variability of a frequency estimate by essentially converting the log-odds to a z-score.\n",
        "\n",
        "In this homework you will implement this ratio for a dataset of your choice to characterize the words that differentiate each one."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries NLTK and PDF plumber\n",
        "# download the packages first here\n",
        "!pip install nltk\n",
        "!pip install PyPDF2\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "import requests # Download from github\n",
        "from PyPDF2 import PdfReader # PDF reader\n",
        "from io import BytesIO\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpYCtGGOomOZ",
        "outputId": "2a2784f6-4bb2-4ac0-89d1-114473ab9cee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7sezbdfE9Ty"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "Your first job is to find two datasets with some interesting opposition -- e.g., news articles from CNN vs. FoxNews, books written by Charles Dickens vs. James Joyce, screenplays of dramas vs. comedies.  Be creative -- this should be driven by what interests you and should reflect your own originality. **This dataset cannot come from Kaggle**.  Feel feel to use web scraping (see [here](https://github.com/CU-ITSS/Web-Data-Scraping-S2023) for a great tutorial) or manually copying/pasting text.  Aim for more than 10,000 tokens for each dataset.\n",
        "   \n",
        "Save those datasets in two files: \"class1_dataset.txt\" and \"class2_dataset.txt\"\n",
        "\n",
        "**Describe each of those datasets and their source in 100-200 words.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_txt(url, output_file):\n",
        "  # Modify the URL to get the raw content from GitHub\n",
        "  response = requests.get(url)\n",
        "  reader = PdfReader(BytesIO(response.content))\n",
        "  text = \"\\n\".join(page.extract_text() or \"\" for page in reader.pages)\n",
        "  with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "      f.write(text)"
      ],
      "metadata": {
        "id": "jnRV9cRmsDGk"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and save the dataset in txt files\n",
        "pdf_to_txt(\"https://raw.githubusercontent.com/willie84/anlp-assignments-2025/main/The_Constitution_of_Kenya_2010.pdf\", \"class1_dataset.txt\")\n",
        "pdf_to_txt(\"https://raw.githubusercontent.com/willie84/anlp-assignments-2025/main/SAConstitution-web-eng.pdf\", \"class2_dataset.txt\")"
      ],
      "metadata": {
        "id": "u2u2f4vJqFfK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gv2ceTy9ogTY"
      },
      "source": [
        "Type your response here:\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The two datasets are the constitutions of Kenya and South Africa. The Kenyan constitution has been obtained as a PDF document from the Kenyan Parliament website here: https://www.parliament.go.ke/sites/default/files/2017-05/The_Constitution_of_Kenya_2010.pdf\n",
        ".\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The South African constitution has also been obtained as a PDF from the South African Department of Justice website here: https://www.justice.gov.za/constitution/SAConstitution-web-eng.pdf\n",
        ". I have downloaded the PDFs and hosted them on my GitHub to be able to download them and use the PDF library to read the PDF content and convert it to text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO4_1a4EE9Ty"
      },
      "source": [
        "## Part 2\n",
        "\n",
        "Tokenize those texts by filling out the `read_and_tokenize` function below (your choice of tokenizer). The input is a filename and the output should be a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6eYQtYiCE9Tz"
      },
      "outputs": [],
      "source": [
        "def read_and_tokenize(filename: str) -> list[str]:\n",
        "  \"\"\"Read the file and output a list of strings (tokens).\"\"\"\n",
        "\n",
        "  from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "  # Read file\n",
        "  with open(filename, \"r\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "  # use NLTK word tokenize for tokenizing\n",
        "  tokens = word_tokenize(text)\n",
        "  return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "z28xc1Y1E9Tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22261c9c-d065-4f6b-e692-f3ff5cfdcab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['LAWS', 'OF', 'KENYA', 'THE', 'CONSTITUTION', 'OF', 'KENYA', ',', '2010', 'Published']\n",
            "['The', 'Constitution', 'OF', 'THE', 'REPUBLIC', 'OF', 'SOUTH', 'AFRICA', ',', '1996']\n",
            "61957\n",
            "59557\n"
          ]
        }
      ],
      "source": [
        "# change these file paths to wherever the datasets you created above live.\n",
        "class1_tokens = read_and_tokenize(\"class1_dataset.txt\")\n",
        "class2_tokens = read_and_tokenize(\"class2_dataset.txt\")\n",
        "print(class1_tokens[:10])\n",
        "print(class2_tokens[:10])\n",
        "print(len(class1_tokens))\n",
        "print(len(class2_tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPHj4k4tE9Tz"
      },
      "source": [
        "## Part 3\n",
        "\n",
        "Now let's find the words that characterize each of those sources (with respect to the other). Implement the log-odds ratio with an uninformative Dirichlet prior. This value, $\\widehat\\zeta_w^{(i-j)}$ for word $w$ reflecting the difference in usage between corpus $i$ and corpus $j$, is given by the following equation:\n",
        "\n",
        "$$\n",
        "\\widehat{\\zeta}_w^{(i-j)}= {\\widehat{d}_w^{(i-j)} \\over \\sqrt{\\sigma^2\\left(\\widehat{d}_w^{(i-j)}\\right)}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\widehat{d}_w^{(i-j)} = \\log \\left({y_w^i + \\alpha_w} \\over {n^i + \\alpha_0 - y_w^i - \\alpha_w}) \\right) -  \\log \\left({y_w^j + \\alpha_w} \\over {n^j + \\alpha_0 - y_w^j - \\alpha_w}) \\right)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma^2\\left(\\widehat{d}_w^{(i-j)}\\right) \\approx {1 \\over {y_w^i + \\alpha_w}} + {1 \\over {y_w^j + \\alpha_w} }\n",
        "$$\n",
        "\n",
        "And:\n",
        "\n",
        "* $y_w^i = $ count of word $w$ in corpus $i$ (likewise for $j$)\n",
        "* $\\alpha_w$ = 0.01\n",
        "* $V$ = size of vocabulary (number of distinct word types)\n",
        "* $\\alpha_0 = V * \\alpha_w$\n",
        "* $n^i = $ number of words in corpus $i$ (likewise for $j$)\n",
        "\n",
        "In this example, the two corpora are your class1 dataset (e.g., $i$ = your class1) and your class2 dataset (e.g., $j$ = class2). Using this metric, print out the 25 words most strongly aligned with class1, and 25 words most strongly aligned with class2.  Again, consult [Monroe et al. 2009, Fighting Words](http://languagelog.ldc.upenn.edu/myl/Monroe.pdf) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lHWahiy8E9T0"
      },
      "outputs": [],
      "source": [
        "def logodds_with_uninformative_prior(tokens_i: list[str], tokens_j: list[str], display=25):\n",
        "    \"\"\"Print out the log odds results given two lists of tokens.\"\"\"\n",
        "    # your code here\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Ddu4uK9pE9T0"
      },
      "outputs": [],
      "source": [
        "logodds_with_uninformative_prior(class1_tokens, class2_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF-MOybbogTZ"
      },
      "source": [
        "To check your work, you can run log-odds on the party platforms from the lab section. With `nltk.word_tokenize` _before_ lower-casing, these should be your top 5 words (and scores, roughly). Depending on your tokenization strategy, your scores might be slightly different.\n",
        "\n",
        "**Democrat**:\n",
        "```\n",
        "president:\t4.75\n",
        "biden:\t4.27\n",
        "to:\t4.11\n",
        "he:\t4.09\n",
        "has:\t4.08\n",
        "```\n",
        "**Republican**\n",
        "```\n",
        "republicans:\t-13.45\n",
        "our:\t-11.23\n",
        "will:\t-10.88\n",
        "american:\t-10.01\n",
        "restore:\t-7.97\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdqSAjQJogTZ",
        "outputId": "26ae13e7-d75d-4753-91a0-8ef02a0b7a08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-09 18:54:45--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_democrat_party_platform.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 283046 (276K) [text/plain]\n",
            "Saving to: ‘2024_democrat_party_platform.txt’\n",
            "\n",
            "2024_democrat_party 100%[===================>] 276.41K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-09-09 18:54:45 (6.49 MB/s) - ‘2024_democrat_party_platform.txt’ saved [283046/283046]\n",
            "\n",
            "--2025-09-09 18:54:45--  https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_republican_party_platform.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 35319 (34K) [text/plain]\n",
            "Saving to: ‘2024_republican_party_platform.txt’\n",
            "\n",
            "2024_republican_par 100%[===================>]  34.49K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-09-09 18:54:46 (3.26 MB/s) - ‘2024_republican_party_platform.txt’ saved [35319/35319]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_democrat_party_platform.txt\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/dbamman/anlp25/main/data/2024_republican_party_platform.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "PywnVLZbogTa"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "logodds_with_uninformative_prior(\n",
        "    [w.lower() for w in nltk.word_tokenize(\"2024_democrat_party_platform.txt\")],\n",
        "    [w.lower() for w in nltk.word_tokenize(\"2024_republican_party_platform.txt\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ttfY-tfogTa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}